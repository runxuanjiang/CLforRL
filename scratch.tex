\documentclass[12pt, letterpaper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[]{amsfonts}
\usepackage{mathtools}
\usepackage{tikz-cd}

\usepackage[]{import}
\usepackage{pdfpages}
\usepackage{xifthen}
\usepackage{transparent}
\usepackage[]{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem*{prop}{Proposition}
\newtheorem*{thm}{Theorem}
\newtheorem*{cor}{Corollary}
\newtheorem*{lemma}{Lemma}
\newtheorem*{rmk}{Remark}

\theoremstyle{remark}
\newtheorem*{note}{Note}
\newtheorem*{exer}{Exercise}
\newtheorem*{eg}{Example(s)}
\newtheorem*{noeg}{Non-Example(s)}

\newcommand{\incfig}[1]{
    \def\svgwidth{0.3\columnwidth}
    \import{./Figures/}{#1.pdf_tex}
}

\renewcommand\qedsymbol{$\blacksquare$}

\usepackage{subfiles}

\title{Curriculum Learning for Reinforcement Learning}
\author{Runxuan Jiang}
\date{}

\begin{document}

\maketitle

\section{Deterministic MDP}

We first consider the case of using curriculum learning on an MDP with deterministic reward and transitions.

\begin{prop}[Existence and bound for deterministic finite MDP.]
    Let \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, H, P, R, \rho)\) be a deterministic MDP with sparse reward, and that \(R(s, a) = \mathcal{X}(s = g)\) where for some \(g \in A\).
    
    Then there exists a finite sequence of $H$ MDP's \(\mathcal{M}_i = (\mathcal{S}, \mathcal{A}, H, P_i, R_i, \rho_i)\) such that the following algorithm

    \begin{enumerate}
        \item Initialize \(\pi_0\) to be a random policy.
        \item For \(1 \leq i \leq H\), Follow \(\pi_{i-1}\) with an \(\epsilon_i\)-greedy policy until an optimal policy is found.
    \end{enumerate}

    will find an optimal policy for \(\mathcal{M}\) in \(O(H^2k)\) episodes in expectation, where \(k = |\mathcal{A}|\).
    
\end{prop}

\begin{proof}[Proof]
    Let \(s_0, s_1, \ldots, s_H = g\) be a trajectory that the optimal policy for \(\mathcal{M}\) takes.
    Construct \(\mathcal{M}_i\) such that \(R_i(s, a) = \mathcal{X}(s = s_i)\) and \(P_i(s_i | s_i, a) = 1 \; \forall a \in A\), otherwise \(P_i = P\) for \(1 \leq i \leq H\).

    Let \(p_i(\epsilon)\) denote the probability that the \(\epsilon\)-greedy algorithm will achieve the optimal policy for MDP \(\mathcal{M}_i\). Then we have 

    \[p_i(\epsilon) \geq f(\epsilon) := (1-\epsilon)^{i-1}(\frac{\epsilon}{k}).\]

    This comes from the fact that the optimal trajectory can be achieved by playing the greedy option \(i-1\) times, and then choosing the exploratory option and choosing the action that reaches the correct state, which has probability \(\frac{\epsilon}{k}\).

    For \(i \geq 2\), \(f\) is concave, so we can find the maximal probability by setting the derivative to 0.

    \begin{note}[]
        \color{red}
        Will need a prove on why this is concave.
    \end{note}

    \[p_i'(\epsilon) = 0\]
    \[\Rightarrow \frac{(1 - \epsilon)^{i-1}}{k} - (i-1)(1-\epsilon)^{i-2}(\frac{\epsilon}{k}) = 0\]
    \[\Rightarrow \frac{(1-\epsilon)^{i-1}}{k} = (i-1)(1-\epsilon)^{i-2}(\frac{\epsilon}{k})\]
    \[\frac{1-\epsilon}{k} = (i-1)(\frac{\epsilon}{k}) \Rightarrow \epsilon = \frac{1}{i}.\]

    By plugging in this value for \(\epsilon\) in \(f\) for \(\mathcal{M}_i\), \(p\) is bounded below by
    \[(1 - \frac{1}{i})^{i-1}\frac{1}{ki}.\]

    So the expected number of episodes to reach the optimal policy for \(\mathcal{M}_i\) is

    \[\frac{1}{(1-\frac{1}{i})^{i-1}(\frac{1}{ki})}\]
    \[= \frac{ki}{(1-\frac{1}{i})^{i-1}} = (i-1)k(\frac{i}{i-1})^i\]

    Notice that since \(i \geq 2\) and \(x \mapsto (\frac{x}{x-1})^x\) is decreasing for \(x > 0\), the above expression is bounded above by

    \[4k(i-1).\]

    So the number of episodes for the full curriculum is in expectation
    \[\sum_{j=2}^{H} 4k(j-1) \leq \frac{H}{2}(4kH)\]

    which is \(O(kH^2)\).
\end{proof}

\begin{rmk}[Tighter bound]
    Note that the above proposition only considers the case where for each \(\mathcal{M}_i\), the \(\epsilon\)-greedy algorithm is greedy for \(i-1\) steps, and then gets the correct state for the single final exploratory step. However, the curriculum learning also helps in the situation where some of the steps are exploratory and "lucky" enough to get the same action as the optimal policy, while the remaining steps are greedy.

    However, even using this the bound is still $O(kH^2)$, so this is the tightest bound we can achieve under these conditions (probably).
\end{rmk}

\begin{proof}[Proof]
    Using a similar setup as the above proof, we now consider using the probability for achieving an optimal policy:

    \[p(\epsilon) \geq f(\epsilon) := {i-1 \choose 0} (1-\epsilon)^{i-1}(\frac{\epsilon}{k}) + {i-1 \choose 1}(1-\epsilon)^{i-2}(\frac{\epsilon}{k})^2 + \ldots + {i-1 \choose i-1} (\frac{\epsilon}{k})^j\]
    \[= (\frac{\epsilon}{k})\sum_{j=0}^{i-1}{i-1 \choose j} (1-\epsilon)^{i-1-j}(\frac{\epsilon}{k})^{j}\]
    \[= (\frac{\epsilon}{k})(1-\epsilon + \frac{\epsilon}{k})^{i-1}.\]

    Note that this is concave on \((0, 1)\).
    \begin{note}[]
        \color{red}
        Proof of concavity needed.
    \end{note}

    Since it is concave, we can set the derivative to 0 to find the \(\epsilon\) that maximizes \(f\). We have
    \[f'(\epsilon) = \frac{(1-\epsilon + \frac{\epsilon}{k})^{i-1}}{k} + (\frac{\epsilon}{k}) (i-1) (1 - \epsilon + \frac{\epsilon}{k})^{i-2}(\frac{1}{k} - 1) = 0\]
    \[\Rightarrow \frac{(1 - \epsilon + \frac{\epsilon}{k})^{i-1}}{k} = (1-i)(\frac{\epsilon}{k})(\frac{1}{k} - 1)(1 - \epsilon + \frac{\epsilon}{k})^{i-2}\]
    \[\Rightarrow (1 - \epsilon + \frac{\epsilon}{k})^{i-1} = \epsilon (1-i) (\frac{1}{k} - 1) (1 - \epsilon+ \frac{\epsilon}{k})^{i-2}\]
    \[\Rightarrow 1 - \epsilon + \frac{\epsilon}{k} = \epsilon (1-i) (\frac{1}{k} - 1)\]
    \[\Rightarrow 1 - \epsilon + \frac{\epsilon}{k} = \frac{\epsilon}{k} - \epsilon + \frac{\epsilon i}{k} + i \epsilon\]
    \[\Rightarrow 1 = \epsilon(i - \frac{i}{k})\]
    \[\Rightarrow \epsilon = \frac{1}{i - \frac{i}{k}} = \frac{k}{i(k-1)}\]

    We then plug this value of \(\epsilon\) back into \(f\) to get the maximal point of \(f\):
    \[(\frac{\frac{k}{i(k-1)}}{k})(1 - \frac{k}{i(k-1)} + \frac{\frac{k}{i(k-1)}}{k})^{i-1} = \frac{1}{i(k-1)}(1 - \frac{k-1}{i(k-1)}) = \frac{1 - \frac{1}{i}}{i(k-1)}\]
    and take the reciporocal to get the expected number of episodes for \(\mathcal{M}_i\), which is
    \[\frac{i(k-1)}{1 - \frac{1}{i}} = \frac{i^2(k-1)}{i-1}\].

    Note that \(x \leq \frac{x^2}{x-1} \leq 4x\), so since \(2 \leq i \leq H\) we get that
    \[\sum_{j = 2}^H \frac{j^2 (k-1)}{j-1} = (k-1 \sum_{j=2}^H \frac{j^2}{j-1})\]
    which is \(O(kH^2)\).

    To prove this is a tight bound, we can show that the probability given by \(p(\epsilon)\) is a tight lower bound to the actual probability.
\end{proof}

\begin{prop}[Existence and bound for deterministic finite MDP]
    Let \(\mathcal{M} = (S, A, H, P, R, \rho)\) be a deterministic MDP with sparse reward, and that \(R(s, a) = \mathcal{X}(s = g)\) for some \(g \in A\).
    
    Then there exists a finite sequence of $H$ MDP's \(\mathcal{M}_i = (S, A, H, P_i, R_i, \rho_i)\) such that the following algorithm

    \begin{enumerate}
        \item Initialize \(\pi_0\) to be a random policy.
        \item For \(1 \leq i \leq H\), Follow \(\pi_{i-1}\) with an \(\epsilon_i\)-greedy policy until an optimal policy is found.
    \end{enumerate}

    will find an optimal policy with probability \(1 - \delta\) in at least \(x\) episodes.
\end{prop}

\begin{note}[]
    \color{red}
    TODO: Prove this theorem (same as above but finding the number of episodes to get an optimal policy with high probability).
\end{note}

\section{Curriculum Learning with Model-Based algorithms}

Given a model-based reinforcement learning algorithm (such as RMAX or UCB), we want to show that we can solve MDP's using the curriculum learning paradigm on the MDP's used by the algorithm to "model" the MDP to be solved. Formally, such algorithms generate a sequence of MDP's \(M_1, \ldots, M_n\) with corresponding optimal policies \(\pi^*_1, \ldots, \pi^*_n\). We want to show that by using \(\pi^*_{i-1}\) with \(\epsilon\)-greedy on \(M_i\) we can achieve optimal regret bounds for \(M_i\). Through induction, we therefore have a "curriculum" for learning the MDP.

\subsection{RMAX}

We start with an episodic nonstationery MDP \(M = (\mathcal{S}, \mathcal{A}, H, \mathbb{P}, R, \rho)\). The R-max algorithm works by encouraging exploration on "unknown" states by setting the reward for those states to a maximal value \(R_{max}\).

\subsubsection{Original R-Max Algorithm}

We first describe the RMAX algorithm for MDP's:

\begin{algorithm}[H]
\caption{R-max for MDP's}
\label{alg:rmax}
\begin{algorithmic}[1]
    \Require MDP \(M = (\mathcal{S}, \mathcal{A}, H, \mathbb{P}, R, \rho), \epsilon > 0, 0 \leq\delta \leq 1, R_{max} > 0\).
    \Procedure{R-max}{$M, \epsilon, \delta, R_{max}, K_1, K_2$}
        \State $Rewards[S][A][H]$ be a new array.
        \State $Transitions[S][A][H]$ be a new array.
        \State $Known[S][A][H]$ be a new array.
        \State $\hat{M} \gets M$ with all states replaced by absorbing states with reward \(R_{max}\) for any action.
        \State \(Done \gets False\)
        \While{not $Done$}
            \State \(Done \gets True\)
            \State \(\pi \gets \) optimal policy for \(\hat{M}\)
            \While{$\pi$ has not been run for more than $K_1$ steps}
                \State obtain trajectories by running \(\pi\) on \(M\).
                \If{a state $(s, a, h)$ has been visited more than $K_2$ times and $Known[s][a][h]$ is false}
                    \State \(Known[s][a][h] \gets True\)
                    \State set the estimated transition probabilities \(Transitions[s][a][h]\) and rewards \(Rewards[s][a][h]\) based on the average of the observed transitions and rewards for \((s, a, h)\)
                    \State replace the transition probabilities and reward in \((s, a, h)\) in \(\hat{M}\) by the estimated values \(Transitions[s][a][h]\) and \(Rewards[s][a][h]\)
                    \State \(Done \gets False\)
                    \State \textbf{break}
                \EndIf
            \EndWhile
        \EndWhile
        \State \textbf{return} The optimal policy for \(\hat{M}\)

    \EndProcedure
    
\end{algorithmic}
\end{algorithm}

\begin{note}[]
    \color{red}
    In the original R-max algorithm, the optimal policy for each \(M_j\) is not used since \(M_j\) is not know fully. Instead, the optimal policy for the estimated MDP for \(M_j\), which is based on the samples collected previously, is used.
\end{note}

\begin{rmk}[Is this curriculum learning?]
    Notice that this algorithm appears similar to curriculum learning. However there are some differences. In curriculum learning, the algorithm is given a sequence of tasks and trains an agent on the current task by using a (near)-optimal policy for the previous task. So given a sequence of MDP's \(M_1, \ldots, M_n\) you explore \(M_j\) with the aid of the policy \(\pi_{j-1}\). However, in the R-max algorithm we are exploring \(M\) directly using \(\pi_j\).
\end{rmk}

\subsubsection{R-max Algorithm with Curriculum Learning}

We first introduce some definitions. Define \(\mathcal{L} = \mathcal{P}(\mathcal{S} \times [H])\) to be the set of all possible sets of (nonstationery) states.

\begin{defn}[]
    Let \(M\) be a nonstationery finite horizon MDP as defined above and let \(L \in \mathcal{L}\). Define the MDP \underline{\(M_L\)} to be the MDP with the same states and actions as \(M\), and with the same rewards for all actions for all states \((s, h) \notin L\). For \((s, h) \in L\), the state is an absorbing state and the reward is set to \(R_{max}\) for all actions.
\end{defn}

We now introduce the curriculum learning algorithm utilizing the exploration principles of the R-max algorithm. For simplicity, we assume that the starting state for \(M\), \(s_0\) is fixed: \(M = (\mathcal{S}, \mathcal{A}, H, \mathbb{P}, R, s_0)\). We also assume that we have access to the MDP \(M_L\) for all \(L \in \mathcal{L}\).

\begin{algorithm}[H]
    \caption{Curriculum Learning with R-max}
    \label{alg:curr_rmax}
\begin{algorithmic}[1]
    \Require MDP $M = (\mathcal{S}, \mathcal{A}, H, \mathbb{P}, R, s_0)$, $\epsilon > 0$, $\delta \leq 1$, $R_{max} > 0$ and $\mathcal{M} = \{M_L: L \in \mathcal{L}\}$.
    \State Initialize $L_0 = (\mathcal{S} \times [H]) \setminus \{(s_0, 1)\}$; $M_0 = M_{L_0}$; $\Phi_0 = \{\bar\pi_0\}$ for $\bar\pi_0$ being uniformly random policy; $(\bar s_0, \bar h_0) = (s_0, 1)$.
    \For{$t = 1, \dots, S \times H$}
        \State Let $\pi_t =$ mixture of $\Phi_{t-1}$ with $\epsilon$-greedy exploration.
        \State $\mathcal{H}_t = \{\}$.
        \While{True}
            \State Run $\pi_t$ on $M_{t-1}$ and add the trajectory to $\mathcal{H}_t$.
            \If{$N_{\bar s, \bar h}(\mathcal{H}) \geq K_3$ for all $(s, h \in (\mathcal{S} \times [H]) \setminus L_{t-1})$}
                \State \textbf{break}
            \EndIf
        \EndWhile
        \State Call offline policy optimization on $\mathcal{H}_t$ that returns $\bar \pi_{t}$, a near-optimal policy for $M_{t-1}$.
        \State {If \# $\bar \pi_t$ will visit some $s, h \in L_{t-1}$ with the highest probability. Set this $s, h$ to be $\bar s_t, \bar h_t$. Otherwise, terminate}
        % \If{$i = K_1$}
        %     \State Output $\bar \pi_t$; BREAK \quad              \textit{\# Found a near-optimal policy}
        % \EndIf
        \State $\Phi_t = \Phi_{t-1} \cup \{\bar \pi_{t-1}\}$.
        %\State Run $\pi_t$ on $M_{t-1}$ until some state $(\bar s_t, \bar h_t) \in L_{t-1}$ is visited $K_3$ times and $(\bar s_{t-1}, \bar h_{t-1})$ is visited $K_2$ times. The trajectories are stored in $\mathcal{H}_t$.
        \State $L_t = L_{t-1} / (\bar s_t, \bar h_t)$; $M_t = M_{L_t}$.
    \EndFor
    \State Output $\bar \pi_t$.
\end{algorithmic}
\end{algorithm}



We now introduce the main theoretical guarantee for the above algorithm.

\begin{thm}[]
    Given a nonstationery finite horizon MDP \(M\) with fixed initial state, an \(\epsilon > 0\), a \(0 \leq \delta \leq 1\), the maximum reward for \(M\), \(R_{max}\) and the \(\mathcal{M}\) corresponding to \(M\), running algorithm \ref{alg:curr_rmax} with \(K_1, K_2, K_3\), the algorithm will output an \(\epsilon\)-optimal policy with probability \(1 - \delta\). Furthermore, the sample complexity of the algorithm is \(\_\).
\end{thm}

We start off with some preliminary results about estimating and MDP.

\begin{thm}[]
    Let \(M\) be a MDP and let \(M'\) be another MDP such that \(||P_{M} - P_{M'}||_{\infty} < \frac{\epsilon}{SH^2R_{max}}\). Then for all policies \(\pi\),
    \[|V_{M}(\pi) - V_{M'}(\pi)| \leq \epsilon\]
\end{thm}

\begin{proof}[Proof]
    Lemma 4 of the R-max paper.
\end{proof}

\begin{lemma}[Justification for \(K_2\)]
    Let \(M\) be an MDP. To approximate the transition probabilility for a state \(s, a, h\) in \(M\) with error less than \(\epsilon\) with probability higher than \(1 - \delta\), we need \(n \geq \frac{-\ln(\delta/2S)}{2\epsilon^2}\) samples.
\end{lemma}

\begin{proof}[Proof]
    Let \(n\) be the number of samples collected where the state \((s, a, h)\) leads to the state \(s'\). Let \(X_i\) be an indicator variable that is 1 if \((s, a, h)\) leads to \((s', h+1)\) and 0 otherwise in \(i\)th sample. Let \(p\) be the actual transition probability of transitioning from \((s, a, h)\) to \((s', h+1)\). Let \(S = \frac{X_1 + \ldots + X_{n}}{n}\). Note that \(0 \leq \frac{X_i}{n} \leq \frac{1}{n}\). Since there are \(S\) possibilities for \((s', h)\), we want the probability that \(|S - p| \geq \epsilon\) to be less than \(\frac{\delta}{S}\). By Hoeffding's inequality we have
    \[P(|S - p| \geq \epsilon) \leq 2 \exp (\frac{-2\epsilon^{2}}{n(\frac{1}{n})^2}) = 2 \exp(-2n\epsilon^2).\]
    We want
    \[2 \exp(-2n\epsilon^2) \leq \delta/S\]
    \[\Rightarrow -2n\epsilon^2 \leq \ln(\frac{\delta}{2S})\]
    \[\Rightarrow n \geq \frac{-\ln(\frac{\delta}{2S})}{2\epsilon^2}.\]
\end{proof}

\begin{lemma}[]
    Let \(\pi\) be the policy defined in line 13 of algorithm \ref{alg:curr_rmax}. Then either \(\pi\) has a probability of accessing a state in \(L\) with probability \(\_\) or the current optimal policy for \(\hat{M}\) is already \(\_\)-optimal for \(M\).
\end{lemma}

\begin{lemma}[Justification for \(K_{1, 1}\) and \(K_3\)]
    Let \(\pi\) be the policy defined in line 13 of algorithm \ref{alg:curr_rmax}. Then we must run \(\pi\) on \(M_i\) for at least \(K_1 = \_\) trajectories to ensure that the following is true:
    \begin{itemize}
        \item If no state \((s', h')\) has been reached \(K_3\) times then there is \(1 - \delta\) probability that the current policy for \(\hat{M}\) is optimal for \(M\).
        \item If a state \((s', h')\) has been reached more than \(K_3\) times then there is \(1 - \delta\) probability that the probability of reaching that state using \(\pi\) on \(M_i\) is at least \(\_\).
    \end{itemize}
    
\end{lemma}

\begin{lemma}[Justification for \(K_{1, 2}\)]
    By running the loop in line 14 of algorithm \ref{alg:curr_rmax} \(K_{1, 2} = \_\) times we guarantee with probability \(1 - \delta\) that the state \((s, h)\) will be reached \(K_2\) times in \(M_i\).
\end{lemma}


\begin{lemma}[]
    Let \(\alpha > 0\) and \(1 \leq j \leq SA\). Denote \((s, a, h)\) to be the entry in \(L_{j-1}\) but not in \(L_{j}\). Consider playing the policy \(\pi_{j-1}^*\) with \(\epsilon\)-greedy on \(M_j\). Let \(V_{M}(\pi)\) denote the value function on MDP \(M\) of policy \(\pi\), and let \(V^*_{M}\) be the value of any optimal policy on \(M\). Then either

    \begin{enumerate}
        \item \(V_{M_j} (\pi_{j-1}) > V^*_{M_j} - \alpha\)
        \item \((s, a, h)\) will be played with probability of at least \(\frac{\alpha}{H*R_{max}}\).
    \end{enumerate}
    
\end{lemma}

\begin{proof}[Proof]
    This proof follows the proof of lemma 6 in the original R-max paper.

    Suffices to show that if (2) does not hold then (1) holds. Suppose that (2) does not hold. Let \(T\) be the set of all possible trajectories on \(M_{j}\) and let \(T'\) be the set of all trajectories that pass through \((s, a, h)\). Let \(P_M^{\pi}(t)\) denotes the probability that the trajectory \(t\) will be reached when playing policy \(\pi\) on the MDP \(M\).  Since (2) does not hold we have that
    \[\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) < \frac{\alpha}{H R_{max}}.\]
    We want to show that
    \[V^*_{M_j} - V_{M_j}(\pi_{j-1}) < \alpha.\]
    Note that the only difference between \(M_j\) and \(M_{j-1}\) is that in \(M_{j-1}\), the state \((s, h)\) is an absorbing state and any action from this state gives a reward of \(R_{max}\), but in \(M_j\) the reward is replaced by the actual reward and the transition probabilities are also replaced by the actual transition probabilities from \(M\). Thus we have
    \[V^*_{M_j} \leq V^*_{M_{j-1}}.\]
    \[\Rightarrow V^*_{M_j} - V_{M_j}(\pi_{j-1}) \leq V^*_{M_{j-1}} - V_{M_j}(\pi_{j-1})\]
    We now further decompose each of these value functions over all possible trajectories. Let \(V_{M}(t)\) denote the total reward of going through trajectory \(t\) on MDP \(M\). Then we have
    \[ |V_{M_j}(\pi_{j-1}) - V_{M_{j-1}}^* | = |\sum_{t \in T} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) - \sum_{t \in T} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)|\]
    \[= |\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) + \sum_{t \in T \setminus T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) - \sum_{t \in T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t) - \sum_{t \in T \setminus T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)\]
    \[\leq |\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) - \sum_{t \in T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)| + |\sum_{t \in T \setminus T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) -  \sum_{t \in T \setminus T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)|\]
    Since the rewards and transition probabilities for \(t \in T \setminus T'\) are the same for MDPs \(M_j\) and \(M_{j-1}\), the second expression above is equal to 0. Thus we are left with the first expression
    \[= |\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) - \sum_{t \in T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)|.\]
    Finally note that since the probabilities for all states before step \(h\) have the same transition probability for \(M_j\) and \(M_{j-1}\), we have that \(\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) = \sum_{t \in T'} P_{M_{j-1}}^{\pi_{j-1}}(t)\). We also have that \(\forall t, 0 \leq V_{M_j}(t) \leq HR_{max}\) and \(0 \leq V_{M_{j-1}}(t) \leq HR_{max}\). Thus we have
    \[|\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) - \sum_{t \in T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)| \leq \sum_{t \in T'} P_{M_j}^{\pi_{j-1}} H R_{max}.\]
    
    However from our assumption we have that 
    \[\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) < \frac{\alpha}{H R_{max}}.\]

    Substituting this into the previous expression we have that
    \[V^*_{M_j} - V_{M_j}(\pi_{j-1}) < \alpha\]
    
    Which is the same statement as (1).
\end{proof}






\end{document}