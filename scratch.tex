\documentclass[12pt, letterpaper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[]{amsfonts}
\usepackage{mathtools}
\usepackage{tikz-cd}

\usepackage[]{import}
\usepackage{pdfpages}
\usepackage{xifthen}
\usepackage{transparent}
\usepackage[]{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem*{prop}{Proposition}
\newtheorem*{thm}{Theorem}
\newtheorem*{cor}{Corollary}
\newtheorem*{lemma}{Lemma}
\newtheorem*{rmk}{Remark}

\theoremstyle{remark}
\newtheorem*{note}{Note}
\newtheorem*{exer}{Exercise}
\newtheorem*{eg}{Example(s)}
\newtheorem*{noeg}{Non-Example(s)}

\newcommand{\incfig}[1]{
    \def\svgwidth{0.3\columnwidth}
    \import{./Figures/}{#1.pdf_tex}
}

\renewcommand\qedsymbol{$\blacksquare$}

\usepackage{subfiles}

\title{Curriculum Learning for Reinforcement Learning}
\author{Runxuan Jiang}
\date{}

\begin{document}

\maketitle

\section{Deterministic MDP}

We first consider the case of using curriculum learning on an MDP with deterministic reward and transitions.

\begin{prop}[Existence and bound for deterministic finite MDP.]
    Let \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, H, P, R, \rho)\) be a deterministic MDP with sparse reward, and that \(R(s, a) = \mathcal{X}(s = g)\) where for some \(g \in A\).
    
    Then there exists a finite sequence of $H$ MDP's \(\mathcal{M}_i = (\mathcal{S}, \mathcal{A}, H, P_i, R_i, \rho_i)\) such that the following algorithm

    \begin{enumerate}
        \item Initialize \(\pi_0\) to be a random policy.
        \item For \(1 \leq i \leq H\), Follow \(\pi_{i-1}\) with an \(\epsilon_i\)-greedy policy until an optimal policy is found.
    \end{enumerate}

    will find an optimal policy for \(\mathcal{M}\) in \(O(H^2k)\) episodes in expectation, where \(k = |\mathcal{A}|\).
    
\end{prop}

\begin{proof}[Proof]
    Let \(s_0, s_1, \ldots, s_H = g\) be a trajectory that the optimal policy for \(\mathcal{M}\) takes.
    Construct \(\mathcal{M}_i\) such that \(R_i(s, a) = \mathcal{X}(s = s_i)\) and \(P_i(s_i | s_i, a) = 1 \; \forall a \in A\), otherwise \(P_i = P\) for \(1 \leq i \leq H\).

    Let \(p_i(\epsilon)\) denote the probability that the \(\epsilon\)-greedy algorithm will achieve the optimal policy for MDP \(\mathcal{M}_i\). Then we have 

    \[p_i(\epsilon) \geq f(\epsilon) := (1-\epsilon)^{i-1}(\frac{\epsilon}{k}).\]

    This comes from the fact that the optimal trajectory can be achieved by playing the greedy option \(i-1\) times, and then choosing the exploratory option and choosing the action that reaches the correct state, which has probability \(\frac{\epsilon}{k}\).

    For \(i \geq 2\), \(f\) is concave, so we can find the maximal probability by setting the derivative to 0.

    \begin{note}[]
        \color{red}
        Will need a prove on why this is concave.
    \end{note}

    \[p_i'(\epsilon) = 0\]
    \[\Rightarrow \frac{(1 - \epsilon)^{i-1}}{k} - (i-1)(1-\epsilon)^{i-2}(\frac{\epsilon}{k}) = 0\]
    \[\Rightarrow \frac{(1-\epsilon)^{i-1}}{k} = (i-1)(1-\epsilon)^{i-2}(\frac{\epsilon}{k})\]
    \[\frac{1-\epsilon}{k} = (i-1)(\frac{\epsilon}{k}) \Rightarrow \epsilon = \frac{1}{i}.\]

    By plugging in this value for \(\epsilon\) in \(f\) for \(\mathcal{M}_i\), \(p\) is bounded below by
    \[(1 - \frac{1}{i})^{i-1}\frac{1}{ki}.\]

    So the expected number of episodes to reach the optimal policy for \(\mathcal{M}_i\) is

    \[\frac{1}{(1-\frac{1}{i})^{i-1}(\frac{1}{ki})}\]
    \[= \frac{ki}{(1-\frac{1}{i})^{i-1}} = (i-1)k(\frac{i}{i-1})^i\]

    Notice that since \(i \geq 2\) and \(x \mapsto (\frac{x}{x-1})^x\) is decreasing for \(x > 0\), the above expression is bounded above by

    \[4k(i-1).\]

    So the number of episodes for the full curriculum is in expectation
    \[\sum_{j=2}^{H} 4k(j-1) \leq \frac{H}{2}(4kH)\]

    which is \(O(kH^2)\).
\end{proof}

\begin{rmk}[Tighter bound]
    Note that the above proposition only considers the case where for each \(\mathcal{M}_i\), the \(\epsilon\)-greedy algorithm is greedy for \(i-1\) steps, and then gets the correct state for the single final exploratory step. However, the curriculum learning also helps in the situation where some of the steps are exploratory and "lucky" enough to get the same action as the optimal policy, while the remaining steps are greedy.

    However, even using this the bound is still $O(kH^2)$, so this is the tightest bound we can achieve under these conditions (probably).
\end{rmk}

\begin{proof}[Proof]
    Using a similar setup as the above proof, we now consider using the probability for achieving an optimal policy:

    \[p(\epsilon) \geq f(\epsilon) := {i-1 \choose 0} (1-\epsilon)^{i-1}(\frac{\epsilon}{k}) + {i-1 \choose 1}(1-\epsilon)^{i-2}(\frac{\epsilon}{k})^2 + \ldots + {i-1 \choose i-1} (\frac{\epsilon}{k})^j\]
    \[= (\frac{\epsilon}{k})\sum_{j=0}^{i-1}{i-1 \choose j} (1-\epsilon)^{i-1-j}(\frac{\epsilon}{k})^{j}\]
    \[= (\frac{\epsilon}{k})(1-\epsilon + \frac{\epsilon}{k})^{i-1}.\]

    Note that this is concave on \((0, 1)\).
    \begin{note}[]
        \color{red}
        Proof of concavity needed.
    \end{note}

    Since it is concave, we can set the derivative to 0 to find the \(\epsilon\) that maximizes \(f\). We have
    \[f'(\epsilon) = \frac{(1-\epsilon + \frac{\epsilon}{k})^{i-1}}{k} + (\frac{\epsilon}{k}) (i-1) (1 - \epsilon + \frac{\epsilon}{k})^{i-2}(\frac{1}{k} - 1) = 0\]
    \[\Rightarrow \frac{(1 - \epsilon + \frac{\epsilon}{k})^{i-1}}{k} = (1-i)(\frac{\epsilon}{k})(\frac{1}{k} - 1)(1 - \epsilon + \frac{\epsilon}{k})^{i-2}\]
    \[\Rightarrow (1 - \epsilon + \frac{\epsilon}{k})^{i-1} = \epsilon (1-i) (\frac{1}{k} - 1) (1 - \epsilon+ \frac{\epsilon}{k})^{i-2}\]
    \[\Rightarrow 1 - \epsilon + \frac{\epsilon}{k} = \epsilon (1-i) (\frac{1}{k} - 1)\]
    \[\Rightarrow 1 - \epsilon + \frac{\epsilon}{k} = \frac{\epsilon}{k} - \epsilon + \frac{\epsilon i}{k} + i \epsilon\]
    \[\Rightarrow 1 = \epsilon(i - \frac{i}{k})\]
    \[\Rightarrow \epsilon = \frac{1}{i - \frac{i}{k}} = \frac{k}{i(k-1)}\]

    We then plug this value of \(\epsilon\) back into \(f\) to get the maximal point of \(f\):
    \[(\frac{\frac{k}{i(k-1)}}{k})(1 - \frac{k}{i(k-1)} + \frac{\frac{k}{i(k-1)}}{k})^{i-1} = \frac{1}{i(k-1)}(1 - \frac{k-1}{i(k-1)}) = \frac{1 - \frac{1}{i}}{i(k-1)}\]
    and take the reciporocal to get the expected number of episodes for \(\mathcal{M}_i\), which is
    \[\frac{i(k-1)}{1 - \frac{1}{i}} = \frac{i^2(k-1)}{i-1}\].

    Note that \(x \leq \frac{x^2}{x-1} \leq 4x\), so since \(2 \leq i \leq H\) we get that
    \[\sum_{j = 2}^H \frac{j^2 (k-1)}{j-1} = (k-1 \sum_{j=2}^H \frac{j^2}{j-1})\]
    which is \(O(kH^2)\).

    To prove this is a tight bound, we can show that the probability given by \(p(\epsilon)\) is a tight lower bound to the actual probability.
\end{proof}

\begin{prop}[Existence and bound for deterministic finite MDP]
    Let \(\mathcal{M} = (S, A, H, P, R, \rho)\) be a deterministic MDP with sparse reward, and that \(R(s, a) = \mathcal{X}(s = g)\) for some \(g \in A\).
    
    Then there exists a finite sequence of $H$ MDP's \(\mathcal{M}_i = (S, A, H, P_i, R_i, \rho_i)\) such that the following algorithm

    \begin{enumerate}
        \item Initialize \(\pi_0\) to be a random policy.
        \item For \(1 \leq i \leq H\), Follow \(\pi_{i-1}\) with an \(\epsilon_i\)-greedy policy until an optimal policy is found.
    \end{enumerate}

    will find an optimal policy with probability \(1 - \delta\) in at least \(x\) episodes.
\end{prop}

\begin{note}[]
    \color{red}
    TODO: Prove this theorem (same as above but finding the number of episodes to get an optimal policy with high probability).
\end{note}

\section{Curriculum Learning with Model-Based algorithms}

Given a model-based reinforcement learning algorithm (such as RMAX or UCB), we want to show that we can solve MDP's using the curriculum learning paradigm on the MDP's used by the algorithm to "model" the MDP to be solved. Formally, such algorithms generate a sequence of MDP's \(M_1, \ldots, M_n\) with corresponding optimal policies \(\pi^*_1, \ldots, \pi^*_n\). We want to show that by using \(\pi^*_{i-1}\) with \(\epsilon\)-greedy on \(M_i\) we can achieve optimal regret bounds for \(M_i\). Through induction, we therefore have a "curriculum" for learning the MDP.

\subsection{RMAX}

We start with an episodic nonstationery MDP \(M = (\mathcal{S}, \mathcal{A}, H, \mathbb{P}, R, \rho)\). Let \(L\) be a collection of states and actions \(L \subset \mathcal{S} \times \mathcal{A} \times [H]\) corresponding to the set of states marked unknown in the RMAX algorithm.

\begin{defn}[\(M_L\)]
    Let \(M\) and \(L\) be defined as above. We define \(M_L\) as a MDP with the same states and actions as \(M\), and with the same rewards and transitions for \((s, a, h) \notin L\). For \((s, a, h) \in L\), the the reward will be equal to \(R_{max}\) and the transition will lead to an absorbing state that always gives reward \(R_{max}\).
\end{defn}

We first describe the RMAX algorithm for MDP's:

\begin{algorithm}[H]
\caption{R-max for MDP's}
\begin{algorithmic}[1]
    \Require MDP \(M = (\mathcal{S}, \mathcal{A}, H, \mathbb{P}, R, \rho), \epsilon > 0, 0 \leq\delta \leq 1, R_{max} > 0\).
    \Procedure{R-max}{$M, \epsilon, \delta, R_{max}$}
        \State $Rewards[S][A][H]$ be a new array.
        \State $Transitions[S][A][H]$ be a new array.
        \State $Known[S][A][H]$ be a new array.
        \State $L \gets \mathcal{S} \times \mathcal{A} \times [H]$
        \State $M_{curr} \gets M_L$
        \For{$i = 1, \ldots, S*A*H$}
            \While{No new entry is known and the current policy has not been run for more than $K_2$ steps}
                \State Obtain trajectories by running the optimal policy for \(M_{curr}\) on \(M\).
                \If{a state $(s, a, h)$ has been visited more than $K_1$ times}
                    \State Mark \((s, a, h)\) as known
                    \State Set the estimated transition probabilities and rewards based on the average of the observed transitions and rewards for \((s, a, h)\).
                    \State Replace the transition dynamics and reward in \((s, a, h)\) in \(M_{curr}\) by the new estimated dynamics.
                \EndIf
            \EndWhile
        \EndFor
        \State \textbf{Return} The optimal policy for \(M_{curr}\)

    \EndProcedure
    
\end{algorithmic}
\end{algorithm}

\begin{rmk}[Is this curriculum learning?]
    Notice that this algorithm appears similar to curriculum learning. However there are some differences. In curriculum learning, the algorithm is given a sequence of tasks and trains an agent on the current task by using a (near)-optimal policy for the previous task. So given a sequence of MDP's \(M_1, \ldots, M_n\) you explore \(M_j\) with the aid of the policy \(\pi_{j-1}\). However, in the R-max algorithm we are exploring \(M\) directly using \(\pi_j\).
\end{rmk}

The RMAX algorithm works by starting with \(L_0 = \mathcal{S} \times \mathcal{A}\), and using the optimal policy for \(M_{L_0}\) on \(M\). Once enough samples have been collected from a state-action pair \((s, a, h)\), it is removed from \(L_0\) to get \(L_1 = L_0 \setminus \{(s, a, h)\}\), and then using the optimal policy for \(M_{L_1}\). This is done until there are no unknown states left (i.e., \(L\) is empty).

Note that
\[\mathcal{S} \times \mathcal{A} \times [H] = L_0 \supset L_1 \supset \ldots \supset L_{SAH} = \emptyset\]

Denote \(M_{L_j}\) as \(M_j\). The following describes the main theorem.

\begin{thm}[]
    Fix \(0 < \delta < 1, \epsilon > 0\). Then we can find an \(\epsilon\)-optimal policy with probability higher than \(1 - \delta\) by performing the following steps on the sequence of MDP's \(M_1, \ldots, M_{SAH}\) with corresponding optimal policies \(\pi^*_{1}, \pi^{*}_{SAH}\):

    \begin{itemize}
        \item Initialize \(\pi_0\) to be a random policy.
        \item For \(t = 1, \ldots, n\), follow \(\pi_{t-1}\) with \(\epsilon\)-greedy exploration to collect \(K_2\) trajectories. Compute the optimal policy \(\pi_t\) from the model learned by the trajectories.
        \item Output \(\pi_{SAH}\).
    \end{itemize}
    
\end{thm}


\begin{lemma}[]
    Let \(\alpha > 0\) and \(1 \leq j \leq SA\). Denote \((s, a)\) to be the entry in \(L_{j-1}\) but not in \(L_{j}\). Consider playing the policy \(\pi_{j-1}^*\) with \(\epsilon\)-greedy on \(M_j\). Let \(V_{M}(\pi)\) denote the value function on MDP \(M\) of policy \(\pi\), and let \(V^*_{M}\) be the value of any optimal policy on \(M\). Then either

    \begin{enumerate}
        \item \(V_{M_j} (\pi_{j-1}) > V^*_{M_j} - \alpha\)
        \item \((s, a, h)\) will be played with probability of at least \(\frac{\alpha}{H*R_{max}}\).
    \end{enumerate}
    
\end{lemma}

\begin{proof}[Proof]
    This proof follows the proof of lemma 6 in the original R-max paper.

    Suffices to show that if (2) does not hold then (1) holds. Suppose that (2) does not hold. Let \(T\) be the set of all possible trajectories on \(M_{j}\) and let \(T'\) be the set of all trajectories that pass through \((s, a, h)\). Let \(P_M^{\pi}(t)\) denotes the probability that the trajectory \(t\) will be reached when playing policy \(\pi\) on the MDP \(M\).  Since (2) does not hold we have that
    \[\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) < \frac{\alpha}{H R_{max}}.\]
    We want to show that
    \[V^*_{M_j} - V_{M_j}(\pi_{j-1}) < \alpha.\]
    Note that the only difference between \(M_j\) and \(M_{j-1}\) is that in \(M_{j-1}\), the state \((s, h)\) is an absorbing state and any action from this state gives a reward of \(R_{max}\), but in \(M_j\) the reward is replaced by the actual reward and the transition probabilities are also replaced by the actual transition probabilities from \(M\). Thus we have
    \[V^*_{M_j} \leq V^*_{M_{j-1}}.\]
    \[\Rightarrow V^*_{M_j} - V_{M_j}(\pi_{j-1}) \leq V^*_{M_{j-1}} - V_{M_j}(\pi_{j-1})\]
    We now further decompose each of these value functions over all possible trajectories. Let \(V_{M}(t)\) denote the total reward of going through trajectory \(t\) on MDP \(M\). Then we have
    \[ |V_{M_j}(\pi_{j-1}) - V_{M_{j-1}}^* | = |\sum_{t \in T} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) - \sum_{t \in T} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)|\]
    \[= |\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) + \sum_{t \in T \setminus T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) - \sum_{t \in T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t) - \sum_{t \in T \setminus T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)\]
    \[\leq |\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) - \sum_{t \in T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)| + |\sum_{t \in T \setminus T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) -  \sum_{t \in T \setminus T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)|\]
    Since the rewards and transition probabilities for \(t \in T \setminus T'\) are the same for MDPs \(M_j\) and \(M_{j-1}\), the second expression above is equal to 0. Thus we are left with the first expression
    \[= |\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) - \sum_{t \in T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)|.\]
    Finally note that since the probabilities for all states before step \(h\) have the same transition probability for \(M_j\) and \(M_{j-1}\), we have that \(\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) = \sum_{t \in T'} P_{M_{j-1}}^{\pi_{j-1}}(t)\). We also have that \(\forall t, 0 \leq V_{M_j}(t) \leq HR_{max}\) and \(0 \leq V_{M_{j-1}}(t) \leq HR_{max}\). Thus we have
    \[|\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) V_{M_j}(t) - \sum_{t \in T'} P_{M_{j-1}}^{\pi_{j-1}}(t) V_{M_{j-1}}(t)| \leq \sum_{t \in T'} P_{M_j}^{\pi_{j-1}} H R_{max}.\]

    \begin{note}[]
        \color{red}
        Not sure if the above argument is valid for this inequality, but a similar inequality was proved in the paper and the details were omitted.
    \end{note}
    
    However from our assumption we have that 
    \[\sum_{t \in T'} P_{M_j}^{\pi_{j-1}}(t) < \frac{\alpha}{H R_{max}}.\]

    Substituting this into the previous expression we have that
    \[V^*_{M_j} - V_{M_j}(\pi_{j-1}) < \alpha\]
    
    Which is the same statement as (1).
\end{proof}

\begin{lemma}[]
    Fix \(0 < \delta < 1\). Then if we obtain \(K_1 = max((\frac{4SHR_max}{\epsilon})^3, -6\ln^3(\frac{\delta}{6SA^2})) + 1\) samples for \((s, a)\) then the estimate for the transition probability for \((s, a)\) is within \(\frac{\epsilon}{2*SHR_{max}}\) of the actual probabilities.
\end{lemma}

\begin{proof}[Proof]
    Use Chernoff bound and pigeonhole principle.
\end{proof}

\begin{lemma}[]
    Fix \(0 < \delta < 1\) and \(\epsilon > 0\). Then we can find an \(\epsilon\)-optimal policy with probability higher than \(1-\delta\) for MDP \(M_j\) after collecting trajectories by running policy \(\pi_{j-1}\) with \(\epsilon\)-greedy exploration for \(K_2\) episodes where \(K_2^{\frac{1}{3}} + K_2 \frac{\alpha}{H*R_{max}} > A^2 S K_1\) and computing the optimal policy for that model.
\end{lemma}

\begin{note}[]
    \color{red}
    We may require less than this number since we only have a single unknown state.
\end{note}

\begin{proof}[Proof]
    By the previous two lemmas, this number of steps ensures that \(K_1\) samples are obtained for \((s, a)\) with high (greater than \(1-\delta\)) probability. Thus, we can accurately estimate the MDP \(M_j\) with a TV distance of less than \(\frac{\epsilon}{2*SHR_{max}}\). By lemma 4 in the R-max paper, this means that the optimal value function of the estimated MDP and \(M_j\) are \(\epsilon\) close to each other. So calculate the optimal policy for the estimated MDP to get the resulting policy.
\end{proof}

\begin{note}[]
    \color{red}
    In this case we don't really use \(\epsilon\)-greedy (we set \(\epsilon\) to zero).
\end{note}

\begin{proof}[Proof (of Theorem)]
    Follows from previous lemma.
\end{proof}

\subsection{UCRL}





\end{document}